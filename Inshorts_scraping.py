# -*- coding: utf-8 -*-
"""Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XVfBc39LHERlMLrurstoZiArkb6XtB7e
"""

#!pip install beautifulsoup4

import requests
from bs4 import BeautifulSoup
import pandas as pd

def storedata(soup):
    for data in soup.findAll("div",{"class":"news-card z-depth-1"}):
        #print(dict["headlines"],data.find(itemprop="headline").getText())
        if data.find(itemprop="headline").getText() not in dict["headlines"]:
            #print(data.find(itemprop="headline").getText(),dict["headlines"].index(data.find(itemprop="headline").getText()))
            dict["headlines"].append(data.find(itemprop="headline").getText())
            dict["text"].append(data.find(itemprop="articleBody").getText())
            dict["date"].append(data.find("span",{"clas":"date"}).getText())
            dict["author"].append(data.find("span",{"class":"author"}).getText())
            if data.find("a",{"class":"source"}):
                dict["read_more"].append(data.find("a",{"class":"source"}).get("href"))
            else:
                dict["read_more"].append("None")
    #print(len(dict["headlines"]))
url="https://www.inshorts.com/en/read"
headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',
        'referer': 'https://inshorts.com/en/read',
        'origin': 'https://inshorts.com',
        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        'X-Requested-With': 'XMLHttpRequest',
    }
r=requests.get(url,headers=headers)
soup=BeautifulSoup(r.content,"lxml")
dict={"headlines":[],"text":[],"date":[],"author":[],"read_more":[]}
storedata(soup)
#Start Ajaxing
start_id=soup.findAll("script",{"type":"text/javascript"})[-1].getText().split()[3].strip(";").strip('"')
for i in range(2000):
    print(i,len(dict["headlines"]),start_id)
    ajax_url="https://inshorts.com/en/ajax/more_news"
    payload={"news_offset":start_id,"categopry":""}
    #print(payload)
    try:
        r=requests.post(ajax_url,payload,headers=headers)
        start_id=r.content.decode("utf-8")[16:26]
        soup=BeautifulSoup(r.text.replace('\\',""),"lxml")
        storedata(soup)
    except:
        pass
    if i%1000==0:
        df = pd.DataFrame(dict)
        df.to_csv("data"+str(i/1000)+".csv", index=False)
        dict = {"headlines": [], "text": [], "date": [], "author": [], "read_more": []}

# !(ls -a)

# !(zip -r clown_data.zip /content)

# !(pwd)




#this code will fetch complete article text from hindustantimes,india today and the guardian.
#to add custom site, one should know BeautifulSoup
# import time
# from collections import Counter,OrderedDict
# d=pd.read_csv("/content/data1.0.csv",encoding="latin1")
# d.drop_duplicates(subset=["headlines"],inplace=True)
# d.reset_index(drop=True,inplace=True)
# #d.describe()
# d["ctext"]=[0 for i in range(len(d["text"]))]
# count=0
# headers = {
#         'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0',
#         'Accept': 'application/json, text/javascript, */*; q=0.01',
#         'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
#         'X-Requested-With': 'XMLHttpRequest'
#     }
# for i,head in enumerate(d["headlines"]):
#     print(i)
#     if(i==2000):
#       break
#     if d["read_more"][i]:
#         if len(d["read_more"][i].split("/"))>2:
#             link=d["read_more"][i].split("/")[2]
#             try:
#                 r=requests.get(d["read_more"][i],headers=headers)
#             except:
#                 time.sleep(10)
#             if i%300==0:
#                 #d.to_csv("complete"+str(i)+".csv", index=False)
#                 time.sleep(10)
#             if link=="www.hindustantimes.com":
#                 #r=requests.get(d["read_more"][i])
#                 soup=BeautifulSoup(r.content,"lxml")
#                 try:
#                     txt=soup.find("div",{"itemprop":"articlebody"}).getText()
#                     count=count+1
#                     d["ctext"][i]=txt
#                 except:pass
#             elif link=="indiatoday.intoday.in":
#                 soup=BeautifulSoup(r.content,"lxml")
#                 soup.find("span",{"itemprop":"articleBody"})
#                 txt=""
#                 try:
#                     for s in soup.find("span",{"itemprop":"articleBody"}).findAll("p")[:-3]:
#                         txt=txt+s.getText()
#                     count=count+1
#                     d["ctext"][i]=txt
#                 except:pass
#             elif link=="www.theguardian.com":
#                 soup=BeautifulSoup(r.content,"lxml")
#                 soup.find("div",{"itemprop":"articleBody"})
#                 txt=""
#                 try:
#                     for s in soup.find("div",{"itemprop":"articleBody"}).findAll("p"):
#                         txt=txt+s.getText()
#                     count=count+1
#                     d["ctext"][i]=txt
#                 except:pass
# print(count)
# d.to_csv("complete"+str(i)+".csv", index=False)

